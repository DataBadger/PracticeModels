{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Import libraries\n",
    "# 2 - import data\n",
    "# 3 - assign features and dependable variable\n",
    "# 3 - deal with nulls\n",
    "# 4 - encode categorical data and labels if needed\n",
    "# 5 - split into train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\Sasha\\Desktop\\\\Py\\\\Git\\\\UdemyExerciseML\\Machine Learning A-Z (Codes and Datasets)\\\\Part 1 - Data Preprocessing\\\\Section 2 -------------------- Part 1 - Data Preprocessing --------------------\\\\Python\\\\Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Country   Age   Salary Purchased\n",
       "0   France  44.0  72000.0        No\n",
       "1    Spain  27.0  48000.0       Yes\n",
       "2  Germany  30.0  54000.0        No\n",
       "3    Spain  38.0  61000.0        No\n",
       "4  Germany  40.0      NaN       Yes"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Country</th>\n      <th>Age</th>\n      <th>Salary</th>\n      <th>Purchased</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>France</td>\n      <td>44.0</td>\n      <td>72000.0</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Spain</td>\n      <td>27.0</td>\n      <td>48000.0</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Germany</td>\n      <td>30.0</td>\n      <td>54000.0</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Spain</td>\n      <td>38.0</td>\n      <td>61000.0</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Germany</td>\n      <td>40.0</td>\n      <td>NaN</td>\n      <td>Yes</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# Check data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10 entries, 0 to 9\nData columns (total 4 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   Country    10 non-null     object \n 1   Age        9 non-null      float64\n 2   Salary     9 non-null      float64\n 3   Purchased  10 non-null     object \ndtypes: float64(2), object(2)\nmemory usage: 448.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df.info() # check for nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign X and y variables, X for features and y for the dependable variable\n",
    "X = df.iloc[:, :-1].values # first range for the rows, second range for the columns\n",
    "y = df.iloc[:, -1].values # all rows, only last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['France' 44.0 72000.0]\n ['Spain' 27.0 48000.0]\n ['Germany' 30.0 54000.0]\n ['Spain' 38.0 61000.0]\n ['Germany' 40.0 nan]\n ['France' 35.0 58000.0]\n ['Spain' nan 52000.0]\n ['France' 48.0 79000.0]\n ['Germany' 50.0 83000.0]\n ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X) # features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "print(y) # dependable variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when dealing with nulls, if its a small percentage, remove them, if its quite a bit, you might want to replace missing values with average or mean value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple imputer will replace nulls with average\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\") # assign values to be replaced and what they will be replaced with\n",
    "imputer.fit(X[:, 1:3]) # replace missing data in the selected rows and columns\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3]) # will return new matrix of features, will need to be assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['France' 44.0 72000.0]\n ['Spain' 27.0 48000.0]\n ['Germany' 30.0 54000.0]\n ['Spain' 38.0 61000.0]\n ['Germany' 40.0 63777.77777777778]\n ['France' 35.0 58000.0]\n ['Spain' 38.77777777777778 52000.0]\n ['France' 48.0 79000.0]\n ['Germany' 50.0 83000.0]\n ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X) # check new data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding categorical data (text) using one hot encoding, since there is no relationship in between the countries column, each of them has to be encoded as a separate feature instead of being france = 1 spain = 2 and germany = 3, since if it was that way it will look like there is numerical order to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers=[(\"encoder\", OneHotEncoder(), [0])], remainder=\"passthrough\")# choose ype of encoding and whether to keep other aka non encoded columns, passthrough will make sure that other columns are kept in\n",
    "X = np.array(ct.fit_transform(X)) # assign returned matrix to the X (features), force it to be transformed into a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[1.0 0.0 0.0 44.0 72000.0]\n [0.0 0.0 1.0 27.0 48000.0]\n [0.0 1.0 0.0 30.0 54000.0]\n [0.0 0.0 1.0 38.0 61000.0]\n [0.0 1.0 0.0 40.0 63777.77777777778]\n [1.0 0.0 0.0 35.0 58000.0]\n [0.0 0.0 1.0 38.77777777777778 52000.0]\n [1.0 0.0 0.0 48.0 79000.0]\n [0.0 1.0 0.0 50.0 83000.0]\n [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets encode y (dependable variable) using lable encoding turn into 0 and 1\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y) # 0 represents NO and 1 represent YES, binary outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling before or after train/test split, if we do feature scaling before splitting data we will have average values from test set portion of data leaking into train set, which will compromise our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets split our train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=1) # random state will ensure that we have same random seed, for same output purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n [0.0 1.0 0.0 40.0 63777.77777777778]\n [1.0 0.0 0.0 44.0 72000.0]\n [0.0 0.0 1.0 38.0 61000.0]\n [0.0 0.0 1.0 27.0 48000.0]\n [1.0 0.0 0.0 48.0 79000.0]\n [0.0 1.0 0.0 50.0 83000.0]\n [1.0 0.0 0.0 35.0 58000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.0 1.0 0.0 30.0 54000.0]\n [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0 1 0 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling, ensures that features are used proportionally and one of them does not dominate the others, not required for all the models\n",
    "# standardisation will get values to be between -3 and 3, Normalisation will get values in range 0 to 1\n",
    "# normalisation recomended when we have normal distribution in all of our feature, standardisation works well all of the times (safer to use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler # do not apply feature scaling to our dummy data(country name)\n",
    "sc = StandardScaler()\n",
    "X_train[:, 3:] = sc.fit_transform(X_train[:, 3:]) # .fit computes . transform actually updates data\n",
    "X_test[:, 3:] = sc.transform(X_test[:, 3:]) # for our model to work correctly on the new data we cannot fit it to the test matrix, we will use scaler from train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n [1.0 0.0 0.0 0.566708506533324 0.633562432710455]\n [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]\n [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]\n [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.0 1.0 0.0 -1.4661817944830124 -0.9069571034860727]\n [1.0 0.0 0.0 -0.44973664397484414 0.2056403393225306]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}